'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/docs/use_cases/case3/',title:"Bias and Data Discovery",section:"Use Case",content:"Bias and Data Discovery #    Dataset: The dataset for this use case is still the TCP collection of historical documents. This use case took a random sample of 12,000 documents, and held out 30% using stratified sampling. While the testing set is balanced (1,800 per class), the training set is highly skewed (only 15% before 1642).\n  Classifiers: The classifiers have been constructed to determine whether a document is written after 1642 based on the 500 most common words in the corpus. The effective classifiers can help understand how word usage changed at this critical date that marks the beginning of the English Civil War.\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. Then, in order to figure out whether bias exists in the dataset, we should explore the performances of classifiers on different datasets by expanding the selection panel at right and choosing either \u0026ldquo;training\u0026rdquo; or \u0026ldquo;testing\u0026rdquo; dataset. Let\u0026rsquo;s first look at the performances of each classifier on \u0026ldquo;training\u0026rdquo; dataset by choosing the Classifier Performance view. and we can find that several classifiers achieve nearly perfect performance.\nNow, let\u0026rsquo;s switch to the \u0026ldquo;testing\u0026rdquo; dataset to see whether classifiers perform differently on testing set. In order to see performances of each classifier under all evaluation methods, we can choose the Metrics Parallel view. To make it easier to see, we can switch on the \u0026ldquo;Filter\u0026rdquo; button to filter the classifiers whose performances are worse than the random classifier, and we can also click the \u0026ldquo;accuracy\u0026rdquo; to make sort classifiers by accuracy. On the testing dataset, we see that most classifiers provide high recall but low precision, suggesting that they were unable to successfully account for the class skew in the training set.\nAccording to the view above, it is obvious to see that the SVM100W (a support vector machine using class weights and more regularization) provides the best performance in all metrics other than recall. Let\u0026rsquo;s make more detailed analysis by opening the Confusion Matrix grid view and we can tell that it is true that SVM100W can achieve more balanced performance. As we have found the most balanced classifier: SVM100W, the following analysis will be made around this classifier. We would expect that performance may be biased near the class boundary, as documents written near the boundary year may be similar to those on the other side (unless there was a dramatic change at the boundary). To check this effect, we select the errors of the classifier: and then open the Histogram view and see the selection in a histogram of dates (left view). We can also normalize this histogram (to account for the skewed distribution) and confirm that most errors are in the buckets near the boundary (right view).   One explanation for the errors may be document length: near the civil war, many short documents were written (e.g., legal decrees). To explore this, we create a subset of documents written near the boundary (\u0026ldquo;left-click\u0026rdquo; the instances in \u0026ldquo;1630-1641\u0026rdquo; and then \u0026ldquo;shift + left-click\u0026rdquo; the instances in \u0026ldquo;1642-1649\u0026rdquo;): and also select the shortest documents (open another Histogram view, expand the panel above and select the \u0026ldquo;length-log\u0026rdquo; in \u0026ldquo;Numeric\u0026rdquo; set. Then \u0026ldquo;right-click\u0026rdquo; the instances in \u0026ldquo;5-6\u0026rdquo; and then \u0026ldquo;shift + right-click\u0026rdquo; the instances in \u0026ldquo;6-7\u0026rdquo;): It shows that show that short documents are over-represented in the time period.\n"}),a.add({id:1,href:'/docs/use_cases/case2/',title:"Fairness Assessment",section:"Use Case",content:"Fairness Assessment #    Dataset: We consider a standard test case for fair learning: the Broward County recidivism dataset, popularized by ProPublica. The data set contains 6,172 instances and 14 numeric features (created by one-hot encoding the categorical features in the initial seven feature data set). 20% are held for testing.\n  Classifiers: The task is to predict whether a person will commit a crime within two years (two-year recidivism). Classifiers built for this problem are often unfair in that they skew errors towards racial and gender bias. We consider three classifiers trained on the data, a baseline random forest, and two hand-tuned variants (C3 and Pos).\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. Since we are comparing within a specific category - race, we need to select data by category. To do this, we should open Histogram view and click the arrow at the top right corner of the view. A dropdown menu will appear. Select the category \u0026ldquo;race\u0026rdquo;. In order to see whether classifiers perform fairly when dealing with different races, we should pick two kinds of races to select in Histogram view. Left click the bar labeled \u0026ldquo;Caucasian\u0026rdquo; and right click the one labeled \u0026ldquo;African-American\u0026rdquo; and they will be highlighted with cyan and magenta respectively. Then we can use Selection Performance view to see the fairness achieved by each classifier. We can first see the \u0026ldquo;accuracy\u0026rdquo; achieved by each classifier based on two selection (left view). Then we can choose to display the \u0026ldquo;precision\u0026rdquo; (middle) and \u0026ldquo;recall\u0026rdquo; (right) by selecting relative buttons on the panel. We can find that while Selection Performance view shows similar accuracies for the selections, the precision and recall are very different. In addition, C3 has high precision but low recall for \u0026ldquo;Caucasians\u0026rdquo;, and high recall but low precision for \u0026ldquo;African-Americans\u0026rdquo;. That is, its errors are biased to predict no for \u0026ldquo;Caucasians\u0026rdquo; and yes for \u0026ldquo;African-Americans\u0026rdquo;. While other classifiers make more errors, their errors are more uniformly distributed. To further compare classifiers based on their prediction profile per class, we can apply Confusion Matrix Grid here: To explore further, we consider the effect of gender. We open another Histogram view, expand the panel above, choose \u0026ldquo;gender\u0026rdquo; : Then, we select the subset of female instances by using \u0026ldquo;left-click\u0026rdquo; in a histogram (left view) and intersect this with the African American subset (right view: using the relationship widget). Back to the Selection Performance view and choose to display the \u0026ldquo;recall\u0026rdquo;, we can notice C3 has very 0 recall on this subset of African American females, while other classifiers achieve more balanced performance.   "}),a.add({id:2,href:'/docs/use_cases/case5/',title:"Feature Sensitivity Testing",section:"Use Case",content:"Feature Sensitivity Testing #    Dataset: The data set is a collection of 554 plays written in the Early Modern Period (1470-1660). Five linguistic features are used. We classify plays with one of four genres (Comedy, History, Tragedy and Tragicomedy).\n  Classifiers: Our experiment uses a Support Vector Machine (SVM) classifier trained with class weights to counteract a skewed training distribution. A stratified sample of 20% was removed as the test set. The training set has very few of the underrepresented classes\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. After model training, a feature sensitivity experiment identifies which features contribute to the classifier’s performance. We create a variant of the data set for each feature. In each data variant, small perturbations (positive and negative) are added to its corresponding feature’s value for all entries in the data set. The resulting data sets have twice as many items as the original (one for positive additions to the feature, one for negative ones). Each data variant is run through the classifier. Here, we consider only the testing set. Such experiments are preferred to simply examine model coefficients because they test the effects of the variables near the actual data. Let\u0026rsquo;s first use Standard Metrics view to compare the performance of each classifier: The experiment results show that for feature Negativity (N), the classifier performs much worse than the baseline (accuracy and Mathews correlation). What\u0026rsquo;s more, two features PersonProperty (PP) and DirectAddress (DA) achieve similar performance to the baseline.\nStandard procedure would conclude that the classifier is sensitive to N but not PP and DA. However, closer examination in Boxer reveals otherwise. The Classifier Performance view shows that PP and DA have similar accuracy to the baseline: Then, let\u0026rsquo;s select the correct and incorrect subsets for the baseline and it allows us to compare with the perturbed results ((left click on the lower box, cyan selection; right click on the upper box, magenta selection): To further analyze the performance of classifiers based on selection, we can use Selection Performance view: For DA, the overlaps are substantial, for PP there is less overlap. While PP gets the same number of instances correct, it is correct on different ones, suggesting the model is sensitive to this feature.\n  "}),a.add({id:3,href:'/docs/user_guide/get_started/',title:"Get Started",section:"User Guide",content:"Get started #  The tutorial will introduce the ways of comparing classfiers' performance. It will guide you to load data, choose views and analyze visualizations.\n1. Load dataset\nClick the \u0026lsquo;+\u0026rsquo; at top left to choose the dataset (we use the \u0026lsquo;wine_quality\u0026rsquo; dataset for this tutorial).\n2. Compare classifiers based on overall performance\nThere are multiple ways to compare the accuracy of each classifier. We can directly use Classifier Performance View, where each bar is composed of the correctly classified instances (represented by the blue bar at the bottom) and misclassified instances (represented by the orange bar on top). To get the exact value of accuracy, we can hover on each bar, then the fraction of correctly classified and misclassified instances will show up. We can also use other views to get such information. In order to choose other views, we could expand the \u0026lsquo;Views\u0026rsquo; panel in the left. For accuracy comparison, we can also use Standard Metrics view and Parallel Metrics view. Apart from accuracy, we can also comapre classifiers based on other metrics, including precision, recall, f1, mcc. For example, we can expand the panel of Classifier Performance View to choose the metrics we want. 3. Compare classifiers based on selected subset\nNow we already have a sense the overall performance of each classifier under the entire dataset, and we might want to focus on the comparisons based on the selected subset. Supposed that we are interested in the performance of each classifier in the \u0026lsquo;red wine\u0026rsquo; instances.\nFirst, we can create the subset by using the Histogram view. Let\u0026rsquo;s expand the panel and choose \u0026lsquo;type\u0026rsquo; in the dropdown menu of \u0026lsquo;Categorial\u0026rsquo;. Then click the bar of \u0026lsquo;red\u0026rsquo; to make the selection. .\nNow the selected instances can be showed in all the bars of the system. To directly compare the performance of classifiers under the selected subset, we could use \u0026lsquo;Selection Performance view\u0026rsquo; where displays the accuracy (we can also choose other metrics by expanding the panel of the view) of each classifier based on the selected instances. .\n"}),a.add({id:4,href:'/docs/use_cases/case6/',title:"Model Selection",section:"Use Case",content:"Model Selection #    Dataset: This use case uses Mushroom dataset and considers a testing set of 2,000 (of 8,124) randomly selected instances.\n  Classifiers: We consider adapting a classifier to identify poisonous mushrooms. The initial training of the baseline classifier uses the color feature, but we built a new classifier that does not use this feature. We consider two approaches to imputing the missing color feature by using the mode of the data and training a decision tree to classify the color based on other features. Therefore, in addition to the baseline classifier, we build two new models: mode and smart that use the imputed versions of the color feature.\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. In order to have an overview of the performances of each classifier, we can first choose the Metrics Parallel view. To make it easier to see, we can switch on the \u0026ldquo;Filter\u0026rdquo; button to filter the classifiers whose performances are worse than the random classifier, and we can also click the \u0026ldquo;accuracy\u0026rdquo; to make sort classifiers by accuracy. The Parallel Metrics view shows that the imputed models perform worse than baseline on all metrics. In addition, the smart model performs better than mode on all metrics except recall, which is likely to be important (we don’t want to eat a poisonous mushroom).\nTo understand these differences, we can open the Classifier Performance view and select the instances where the baseline and smart classifiers are incorrect (left-click the instances which are wrongly classified by \u0026ldquo;clean\u0026rdquo; classifier, and right-click the instances being wrongly classified by \u0026ldquo;smart\u0026rdquo; classifier):\nBy expanding the selection panel, we can get the information about the intersection among two selections: we find that the latter is almost a proper subset of the former.\nWe wish to understand if the lower performance of the smart classifier can be attributed to imputation mistakes. We select the instances where the baseline is correct and where smart is wrong, and intersect them: To further explore the detailed features of these sets, we can apply Histogram view here and select the color feature: we see that most of the errors are white mushrooms. Looking at the imputed feature over this set, we see that the smart imputer never labels these as white. These mistakes of the imputer likely cause the misclassifications.\n  "}),a.add({id:5,href:'/docs/use_cases/case1/',title:"Model Selection and Data Discovery",section:"Use Case",content:"Model Selection and Data Discovery #    Dataset: This use case considers a corpus of 59,989 documents from a historical literary collection and the data counts the 500 most common English words in each document.\n  Classifiers: The classifiers for this task are decision tree classifiers using a variety of univariate feature selection strategies, each selecting 10 words to count for features. The feature selection methods were: most relevant by a CHI-squared univariate feature selector (C), most common features (N), randomly chosen features (R), and, as a baseline, the features deemed worst (out of the 500 candidate words) by the CHI-squared test (W).\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. In order to have an overview of performances of each classifier, we can first choose the Metrics Parallel view. The Parallel Metrics view shows a consistent ordering of the classifiers across all metrics: C is slightly better than R and N, which are much better than W.\nThen we can apply Classifier Performance here to see the accuracy details of each classifier: Next, let\u0026rsquo;t make some selections. We can select the mistakes made by the top classifiers (cyan for C’s mistakes, and magenta for X\u0026rsquo;s mistakes): In order to see the error\u0026rsquo;s distribution on different classes, we can open the Histogram view and focus on the class distribution: we find that the errors are relatively evenly distributed among the classes and this is surprising given the skewed training distribution.\nWe also see in the Classifier Performance view that different classifiers make different errors (e.g., only half of C’s errors are made by N): To select challenging instances, or to see if a selected set contains easy items, we can use Cumulative Accuracy view: Overall, the Cumulative Accuracy view shows that there are very few instances that all classifiers were wrong on (2.6%).\nLet\u0026rsquo;s back to Histogram view and focus on the error distribution on specific features. Open a Histogram view of document lengths and drag the slider to change the number o bins: and we can see that performance is relatively consistent over the range of document lengths.\n  "}),a.add({id:6,href:'/docs/use_cases/case4/',title:"Model Selection and Tuning",section:"Use Case",content:"Model Selection and Tuning #    Dataset: The data consists of 5044 movies with 27 features, however, 25% is sequestered for final assessment. A stratified sampling of 200 movies per class is held out from the 3756 for testing.\n  Classifiers: We develop a classifier to predict movie ratings from IMDB. Classifiers predict a movie’s rating (low, medium, high). A variety of classifiers were constructed such as linear regression(lr), k- neural network(knn), support vector machines(svm), neural network(nn), random forest(rf) and linear discriminant analysis(lda), none with acceptable accuracy.\n  Walkthrough:\nFirst, click the button on the top left and load the dataset. Since classifier \u0026ldquo;svm-b\u0026rdquo; is the classifier after tuning and classifier is the classifier which is not necessary for this use case, let\u0026rsquo;s first exclude these two classifiers via the selection panel on the right: In addition, we should make sure we are using the \u0026ldquo;testing\u0026rdquo; dataset: Then, let\u0026rsquo;s use the Classifier Performance view to see the performance of each classifiers: This view confirms the poor performance of classifers.\nIn order to get more detailed information on those challenging instances, we can apply Cumulative Accuracy view: The Cumulative Accuracy view shows large numbers of instances that are easy and hard (all or no classifiers are correct).\nLet\u0026rsquo;s select these easy (left click for cyan) and hard subsets (right click for magenta). Open Histogram view of class distribution: In a Histogram view, we can see the hard elements are in the high class, suggesting a skewed training set as shown above.\nNow, let\u0026rsquo;s switch our dataset from \u0026ldquo;testing\u0026rdquo; to \u0026ldquo;training\u0026rdquo;: Back to the Histogram view of class distribution, we can find that this class distribution confirms the skewed nature: Therefore, a new classifier called \u0026ldquo;svm_w\u0026rdquo; was built that accounts for this skew (now we should include the \u0026ldquo;svm_w\u0026rdquo; classifier in the following analysis). Let\u0026rsquo;s look at the performance of \u0026ldquo;svm_w\u0026rdquo; classifier on \u0026ldquo;testing\u0026rdquo; dataset. We can then left-click the misclassified instances of \u0026ldquo;svm_w\u0026rdquo; in Classifier Performance view and keep the magenta selection being the same as the previous one. The below Histogram view shows that the new classifier has superior performance, although its errors are still biased. In conclusion, while conventional tools can show skew, the example shows how Boxer’s flexible mechanisms allow performance effects to be connected to data issues.\n  "}),a.add({id:7,href:'/docs/introduction/overall_introduction/',title:"Overall Introduction",section:"Introduction",content:"Overall Introduction #  Boxer #  Motivation #  Machine learning practitioners often perform experiments that compare classification results. Users gather the results of different classifiers and/or data perturbations on a collection of testing examples. Results data are stored and analyzed for tasks such as model selection, hyper-parameter tuning, data quality assessment, fairness testing, and gaining insight about the underlying data. Classifier comparison experiments are typically evaluated by summary statistics of model performance, such as accuracy, F1, and related metrics. These aggregate measures provide for a quick summary, but not detailed examination. Examining performance on different subsets of data can provide insights into the models (e.g., to understand performance for future improvement), the data (e.g., to understand data quality issues to improve cleaning), or the underlying phenomena (e.g., to identify potential causal relationships). Making decisions solely on aggregated data can lead to missing important aspects of classifier performance. To perform such closer examination, practitioners rely on scripting and existing tools in their standard workflows. The lack of specific tooling makes the process laborious and comparisons challenging, limiting how often experiments are examined in detail.\nMain Contribution #  Boxer is a comprehensive approach for interactive comparison of machine learning classifier results. It has been implemented in a prototype system. We show how Boxer enables users to perform a variety of tasks in assessing machine learning systems.\nInnovations #    The approach to classifier comparison that combines subset identification, metric selection, and comparative visualization to enable detailed comparison in classifier results.\n  The architecture of multiple selections and set algebra that allows users to flexibly link views and specify data subsets of interest.\n  Interactive techniques and visual designs that make the approach practical. These key ideas should be applicable in other systems for interactive comparison within complex data.\n  Example of Boxer System #  This figure shows how Boxer’s flexible mechanisms can be used to analyze whether a person will commit a crime within two years based on the data set contains 6,172 instances. Parallel Metrics view (A) shows the C3 classifier has better performance by all metrics. A histogram of race (F) selects Caucasian (cyan) and African-American (pink) instances. The Overall Performance view (B) shows C3\u0026rsquo;s overall higher precision, but a lack of overlap with cyan. The Confusion Matrix (C) Grid view shows many false positives for African-Americans and many false negatives for Caucasians for C3. Histograms show the distribution of selected sets across the actual (D) and the C3-predicted class (E). The Performance Selection views in the third line compare accuracy (G) , precision (H) , and recall (I) for C3 on the subsets.\nCBoxer #  Motivation #  Classifier assessment is more challenging when the result is continuously valued, rather than a discrete choice. Classification models often output a continuous valued score for their predictions. Sometimes, these scores are used directly to quantify the quality of the prediction. Even if the scores are ultimately thresholded to provide a binary decision, analysis of the scores can provide useful insights on classifier performance. Therefore, assessment must consider both correctness and score. Current methods address specific tasks. While these tools enhance the baseline of scripting within standard workflows, they do not extend to the diverse range of tasks users encounter, either by adapting to new situations or combining effectively to provide richer analyses.\nMain Contribution #  CBoxer provides a more comprehensive approach using a combination of task-specific solutions and standard views and coordination mechanisms. It support the assessment of continuouslyvalued binary classifiers that is sufficiently flexible to adapt to a wide range of tasks.\nInnovations #   A set of views that support classifier assessment and allow flexible combination to perform detailed analyses. A set of design elements, such as trinary classification, that enable visualizations that readily adapt to interactive comparison. The mechanisms to discourage over-generalization. An example of how thinking in terms of comparison can enable the design of flexible tools that serve a variety of tasks, many of which may not obviously be comparison.  Example of CBoxer #  The CBoxer system assessing three classifiers for the disease prediction problem. (A) Reliability Curve view, (B) Performance Confidence view, (C) Uncertainty Heatmap view, (D) Bandwidth Assessment view, (E) Trinary Distribution view, and (F) Histogram view. The Probability Control panel (G) and Views Control panel (H) were used to configure the display. The user has selected the false positive (magenta) and uncertain (cyan) items for the LR classifier in (E) as indicated by the arrows. These selections can be seen in other views, including (F) that shows that the classifier is more likely to be uncertain for men.\n"}),a.add({id:8,href:'/docs/publication/paper/',title:"Paper",section:"Publication",content:"Paper #  To cite boxer:\n Michael Gleicher, Aditya Barve, Xinyu Yu, and Florian Heimerl. Boxer: Interactive Comparison of Classifier Results. Computer Graphics Forum 39 (3), June 2020. DOI:10.1111/cgf.13972\n Official journal web page (the paper is open access!): https://diglib.eg.org/handle/10.1111/cgf13972\n"}),a.add({id:9,href:'/docs/user_guide/other_usage/',title:"Views Manipulation",section:"User Guide",content:"Additional guide for views manipulation #  This tutorial will walk through some basic usages of Boxer system, like how to make views, save / reload views configuration.\nMake views #  Save and reload configuration #  Clicking the \u0026ldquo;Save Views\u0026rdquo; button enables us to save the configurations of current views. After clicing the button, a \u0026ldquo;views_config.json\u0026rdquo; file will be created and it contains the important configurations of all views, including the view\u0026rsquo;s mode, the selected options in each view, the thresholds of classifiers, etc. Supposed we happen to close the Boxer web site and want to resume all views and configurations saved before, we could click the \u0026ldquo;Reload Views\u0026rdquo;.\n"}),a.add({id:10,href:'/docs/use_cases/case7/',title:"(Continuous) Hyper parameter Tuning",section:"Use Case",content:"Hyper parameter Tuning #    Dataset: The scenario uses wine quality benchmark from UCI Machine Learnng Repository, which requires classifying the quality of a wine from its properties.\n  Classifiers: Because we are trying to determine a hyper-parameter, we perform this analysis using the training data. We build a random forest classifier, and find that it gives good performance over a range of thresholds. We want to understand if changes in the threshold affect the outcomes\n  Walkthrough:\nAfter loading the data, let\u0026rsquo;s first open the Bandwidth Assessment view. We see that the accuracy is constant for thresholds around .5. Then we use the Probability Control panel to adjust the threshold. For several values in the range, we create new classifiers that use the same model with different thresholds (.5, .55, .6, .65) and compare these models. We see that accuracy, F1 and MCC scores are very similar across the four. However, if we select the errors (left click to select the errors when threshold is .5, and right click to select the errors when threshold is .6), we see that they are different: For the lower thresholds, there are more false negatives, while for the higher thresholds more false positives. Trinary Instance Distribution view also shows this. In conclusion, this classifier is quite sensitive to small changes in the threshold, even though those changes do not affect the most common metrics.\n  "}),a.add({id:11,href:'/docs/user_guide/view_description/',title:"Descriptions of Views",section:"User Guide",content:"Description of views #  Overview of Boxer Views #  Boxer features a number of different views that provide often needed arrangements of boxes useful in performing classifier comparison. This part will provide you with an overview of the views in Boxer. You will learn the basic uses of each view.\nTo add a view to the workspace, use the Views dropdown in the right panel of Boxer\u0026rsquo;s window. It is below the \u0026ldquo;filters\u0026rdquo; panel.\nTo see examples of how the views are used in analysis scenarios, see the use cases.\nClassifier Performance View #  The Classifier Performance view: shows the performance of each classifier in a stacked bar. A variety of metrics can be chosen. For metrics that are ratios of subset counts (e.g., accuracy, precision, and recall), the bars are stacks of boxes. This view provides a simple overview of classifier performance, and an easy way to select sets of instances (e.g., what classifier predicts correctly). The view allows sorting by value to facilitate identification and comparison of the best or worst classifiers. Histogram View #  The Histogram view shows the distribution of the data across a feature. This includes the data features as well as the actual and predicted classes. Users can choose to show different distributions. Continuous features are bucketed, and categorical features can also be chosen. Histograms provide an important mechanism for selection as well as comparative display, as bars serve as boxes. Cumulative Accuracy view #  The Cumulative Accuracy view shows how many classifiers correctly labeled each data instance. This view can be used to select challenging instances, or to see if a selected set contains easy items.\nSelection and Per-Class Performance Views #  The Selection Performance view (left) shows the performance of each classifier across both selections. A variety of different metrics can be chosen (accuracy, F1, etc.). The *Per-Class Performance view\u0026amp; (right) shows the performance of each classifier for each of the actual classes of the instances. Confusion Matrix Grid View #  The Confusion Matrix Grid view provides the standard view of classification results for each classifier. It can be used to compare classifiers based on their prediction profile per class. Three additional matrices are also provided by Boxer and serve as a comparison baseline: the oracle, shows the performance of a perfect classifier; the majority classifier, predicts the majority class in the dataset; the random classifier, predicts a random class for each instance. Pairwise Consensus View #  The Pairwise Consensus view shows the agreement and disagreement between each pair of classifiers as a matrix. It conveys the number of instances for which two classifiers predict the same label. The matrix is split on the diagonal to distinguish agreement on correct vs. incorrect instances. This view can be used to identify correlations between classifiers. Standard Metrics and Parallel Metrics Views #  The Standard Metrics view and Parallel Metrics view provide overall statistics about the classifiers. The Standard Metrics view (left) provides a simple table of many metrics across all classifiers. The Parallel Metrics view (right) presents the same information in a parallel coordinates chart, which can identify correlation between metrics. Instance List View #  The Instance List view shows a tabular display of the instances in the active selections. Instances are color-coded to indicate which selections they are part of. Single instances can be selected as the selection in this view, which allows for fine-grained, instance-level modification of selections. Selection Controls Panel #  The Selection Controls panel can support selection display and interaction, as well as allows users to switch between training set, selection set, or both. Selection is a subset of instances of interest to the user. In Boxer, boxes serve to create selections (e.g., clicking on a box sets the active selection), and the active selection can be presented visually in each box to enable comparison by showing the intersection between the box and the selection. The left and right mouse buttons are used to make the first and second selections separately. Consistent coloring is used throughout the interface: cyan represents the first active selection, magenta the second.\nExtra views designed for CBoxer #  The following views are included in CBoxer (Continuous Boxer) .\nPerformance Curves view #  The Performance Curves view shows the performance of each classifier based on a line chart. Either Receiver Operator Characteristic (ROC) curve or Precision-Recall (PR) curve can be chosen by users based on their preference. This view helps users assess model performance via traditional summary statistics and graphs, and also allows users to see how the number of correctly classified positive samples varies with the number of incorrectly classified negative samples. In addition to provide summary statistics of classifiers, this view also enables users to set threshold by clicking any point on curves. The changes of threshold will also be showed on other views. Reliability Curve view #  The Reliability Curve view provides the information of how the predicted probabilities of each classifier match the expected distribution of probabilities for each class . The x axis in this view represents the average predicted probability in each bin, then the y axis is the fraction of positives. The more reliable a forecast, the closer the points will appear along the main diagonal from thebottom left to the top right of the plot. The Reliability Curve view and Performance Confidence view both help assess the calibration. The former allows for quick comparison, while the latter integrates information about the amounts and allows for selection. Trinary Performance Confidence view #  The Trinary Performance Confidence view shows a histogram (bar chart) of the number of items for each binned level of score. Stacked bars are used to show different classifications at each level. This basic design of using a position encoding for score level and color for correctness. Also, we use a vertical design to make better use of space when juxtaposing multiple classifiers for comparison.We bin the items (rather than providing individual marks to scale to large collections of items. The stacked bar charts divide the actual class into areas; the prediction is encoded by position relative to the threshold. Our current design uses colors for correct and incorrect: negative/positive must be inferred by position relative to the threshold. Experiments with separate colors led to an even more busy display. Trinary Bandwidth Assessment view #  The Trinary Bandwidth Assessment view summarizes the performance of a classifier over a range of thresholds and bandwidths. The top graph shows a line graph of accuracy as threshold changes. Three bandwidths are selected for comparison, each is assigned a color. The error-bar-like glyphs show the range of accuracy possible with the different bandwidths depending on how one interprets rejected items. The top of the bar considers rejected items as correct, the bottom as incorrect. The bar chart at the bottom shows the number of rejected items with circles to enable easier selection. Trinary Instance Distribution view #  The Trinary Instance Distribution view provides a summary of the classification decisions for given threshold values, allowing for assessment of a threshold setting. The standard confusion matrix does not work as there is a 5th category (rejected items). Instead, the design uses a stacked bar design to allow for comparison between classifiers. To enable different comparisons against common baselines, the ordering of the stacked elements can be changed. Correctness mode groups right and wrong answers, while score mode groups positive and negative predictions. Uncertainty Heatmap view #  The Uncertainty Heatmap view shows performance for all values of upper and lower thresholds simultaneously. Thresholds are mapped to position, a performance metric (e.g., accuracy or recall) is mapped to color, and the percentage of non-rejected items is mapped to radius. Settings that reject many items are less salient - even if they have favorable colors. Hovering over a circle exposes the numerical values for all metrics. The view allows for quick identification of favorable ranges in terms of performance, while considering rejection rate. Rejection Curve view #  The Rejection Curve view shows the Accuracy-Rejection Curve (ARC) which plots performance against rejection rate (percentage of items rejected), providing access to the threshold levels for different rates of rejection. The Rejection Curve view extends standard ARCs by permitting the use of a variety of metrics beyond accuracy to help the user understand performance trade-offs. It can display multiple curves to allow for comparison between classifiers, different decision threshold levels, or different subgroups. Focus Item view #  In addition to selection 1 (cyan) and 2 (magenta), the system also introduces the notion of Focus Items. A Focus Item is a single data point of interest in the dataset. The Focus Item View provides a way to select Focus Items and examine them. With the help of this view, you can traverse through the entire dataset, or through a subset selected using the dropdown menu. You may also look up random data points. The Focus Item View shows all the details of a data point, including images.\nNote that a focus item appear in other views as a yellow dot. You may find the \u0026ldquo;EMPHASIZE\u0026rdquo; button in the Focus Item View helpful when the focus dots are too small.\n"}),a.add({id:12,href:'/docs/publication/video/',title:"Video",section:"Publication",content:"Boxer Videos #  Boxer (Submission) Video #  Continuous Boxer Video #  to be updated\u0026hellip;\n"}),a.add({id:13,href:'/docs/use_cases/case8/',title:"(Continuous) Data Examination",section:"Use Case",content:"Data Examination #    Dataset: This use case considers a corpus of 59,989 documents from a historical literary collection: Text Creation Partnership (TCP) transcriptions of the Early English Books Online (EEBO). The data counts the 500 most common English words in each document. For the experiment, we took a random sample of 2500 documents, and held out 30% using stratified sampling.\n  Classifiers: We construct classifiers that determine whether a document is written after 1642 based on the 500 most common words in the corpus. We constructed random forest (RF) and logistic regression classifiers (LR), with and without applying corrections for class skew.\n  Walkthrough: While ground truth is known, effective classifiers can help understand how word usage changed at this critical date that marks the beginning of the English Civil War. The collection is skewed (only 25% of the documents were written before 1642). Therefore, we prefer MCC as a correctness metric.\nBased on Performance Overall view, the random forest classifiers achieve worse performance with the default (.5) threshold on the test set. Since we prefer to using MCC as correctness metric, we need to set classifiers' thresholds based on their MCC values. Using the Bandwidth Assessment view we can choose the threshold that optimizes MCC for each classifier. Based on our findings in Bandwidth Assessment view, we set all classifiers' thresholds as .7 (the following graph shows the how MCC changes with threhsold of classifier rf). Now, we can tell that after choosing the threshold that achieves the highest MCC for each classifier, the RF classifiers have the best performance across metrics. However, this optimized performance may be specific to the test set. To provide a check against overfitting, we use CBoxer’s bootstrap sampling feature to create 10 new testing sets from the original (the following graph shows how to use Instance Sampling panel to create samples) and see that the results do not change. While the precise values of threshold and metrics may change, in all cases, RF beats LR. A question is whether performance near the boundary is different than documents written farther from the critical date. We can select the time periods before and after the critical date by using Histogram view Then under the \u0026lsquo;original distribution\u0026rsquo; mode of the Performance Confidence view, we can see that the scores are generally closer to the threshold. Both before and after the boundary seem to be problematic which suggests that language was in flux before the English civil war. Another question is whether the length of documents has any connection to the certainty of a prediction. Using a Histogram view, Let\u0026rsquo;s select the shortest and longest documents: Then we can view the distribution over Performance Confidence view.\nWe can see that long documents are disproportionately given confident scores, while the shortest documents (less than 150 words) score in the center (less confident predictions).\n  "}),a.add({id:14,href:'/docs/use_cases/case9/',title:"(Continuous) Model Selection",section:"Use Case",content:"Data Examination #    Dataset: The scenario uses the income classification benchmark dataset from that has been downsampled.\n  Classifiers: Classifiers determine whether an individual’s income is above a certain level. Three models were constructed with different methods: multi-layer perceptron (MLP), logistic regression (LR), and Na¨ıve Bayes (NB).\n  Walkthrough: From Performance Overall view, the 3 models provide very similar correctness summary scores (F1 is .783, identical to 3 decimal places) over a test set of 1500 items.\nWe prefer a calibrated classifier. The AUC score in Performance Curve suggests LR is slightly better, which is confirmed by the Reliability Curve view.\nHowever this view also shows that there are few items with middle scores, so the curve may be unreliable.\nWe choose to examine the differences in detail to confirm and explore the differences. The Performance Confidence view shows a clear picture: NB provides a sharp distribution with many extreme values. We see that many of the errors have extreme values.\nIn contrast, LR has many scores in the middle. If we select those intances by using Trinary Instance view, we find many of them are errors as well. To explore the difference, we select the items that NB has errors and extreme scores by unioning together the error in the highest and lowest bars of the Performance Confidence view. While LR also gets many of these wrong, its errors tend to be distributed in the middle scores. Moreover, if we want to compare performance by considering rejections, Rejection Curve view shows the F1 score of LR raises above.9 when the rejection rate is fixed around 20%, while the F1 for NB is effectively unchanged. The Uncertainty Heatmap view also show this. By setting the thresholds of LR between .4 and .6 (we get these two values from the analysis in the Rejection Curve view (Fig. 5C)) and selecting the score values between this range, we see this includes the majority of LR errors.   "}),a.add({id:15,href:'/docs/use_cases/case20/',title:"(Continuous) Model Selection and Detail Examination",section:"Use Case",content:"Model Selection and Detail Examinationt #  This use case shows how our approach can help with model selection and threshold tuning. A classifier was built for the CIFAR 100 computer vision benchmark using Tensorflow.\n  Dataset: In this example, we have a dataset of various flower images. The dataset has 100 classes. Our goal is to create a binary classifier for a “meta-class” which combines 5 of the main classes. In particular, we want to classify flowers, which can be any one of 5 of the original classes. Because of this, the dataset is quite imbalanced: flowers are only 5% of the total instances.\n  Classifier: The classifiers were created using three different strategies that combine the base classes: sum, average, and largest. Because of the class imbalance, we use Mathews Correlation (MCC) as the metric. Each combination strategy produces different ranges of scores.\n  Walkthrough:\nAfter loading the dataset, we opened the Reliability Curve view (A), Bandwidth Assessment view (B), Rejection Curve view (C), Trinary Distribution view (E), and Focus Item view (F).\n  Each combination strategy produces different ranges of scores. We can use the Reliability Curve view to see the differences, and estimate appropriate thresholds for each.\n For each model, we use the Bandwidth Assessment view to choose a threshold that provides high MCC yet provides a range of available rejection rates.\n The Rejection Curve view shows that each model gets a performance gain from a 10% rejection rate. After tuning each model appropriately all have similar performance.\n Using the Trinary Distribution view we can make selections to look for differences in the similar performance. We note that while each model rejects the same number of items, they reject different items. While the number of errors is small, they are different between models. One model has more false positives, while the other has more false negatives.\n We select the false positives of the largest model. The Focus Item view allows us to step through these errors to look for patterns. We notice that many of the errors are flowers with insects on them. Because these images are labeled as insect, they are scored as misclassified.\n  "}),a.add({id:16,href:'/docs/user_guide/data_preparation/',title:"Data Preparation",section:"User Guide",content:"Instructions to prepare your own data for Boxer #  CORS #  In order to successfully load your dataset, you need to first add an extension called CORS (Access-Control-Allow-Origin lets you easily perform cross-domain Ajax requests in web applications) to Chrome. Everytime you want to use your own dataset, please remember to enable \u0026lsquo;CORS\u0026rsquo;. Data Set Composition #  The data set used for Boxer consists of 3 files:\n  results.csv: This file contains the prediction results of each classifier on each instance, where each row represents the id of the data sample and each column represents the name of the classifier.   features.csv: This file displays the features of each instance, where each row represents the id of the data sample and each column represents the name of each feature.   manifest.json: This file is the configuration of the data set, which helps Boxer system to read and load the data set.\n  { \u0026quot;datasetName\u0026quot;: \u0026quot;IMDB 5000 dataset\u0026quot;, // name of the data set \u0026quot;classes\u0026quot;: [ // array including the names of classes \u0026quot;HIGH\u0026quot;, \u0026quot;MED\u0026quot;, \u0026quot;LOW\u0026quot; ], \u0026quot;classifiers\u0026quot;: [ // array including the names of classifiers \u0026quot;LR\u0026quot;, \u0026quot;KNN\u0026quot;, \u0026quot;LDA\u0026quot;, \u0026quot;NB\u0026quot;, \u0026quot;SVM\u0026quot;, \u0026quot;SVM-W\u0026quot;, \u0026quot;RF\u0026quot;, \u0026quot;RF-W\u0026quot;, \u0026quot;DT\u0026quot; ], \u0026quot;features\u0026quot;: { // dict including the detailed information of each feature: \u0026quot;type\u0026quot;, \u0026quot;description\u0026quot;, \u0026quot;categories\u0026quot;(optional), \u0026quot;bounds\u0026quot;(optional) \u0026quot;train_or_test\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;categorical\u0026quot;, \u0026quot;categories\u0026quot;: [ \u0026quot;train\u0026quot;, \u0026quot;test\u0026quot; ], \u0026quot;description\u0026quot;: \u0026quot;whether instance was used for training or testing\u0026quot; }, \u0026quot;movie_title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;nominal\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;title\u0026quot; }, \u0026quot;title_year\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;interval\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;year released\u0026quot; }, \u0026quot;content_rating\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ratio\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;content rating\u0026quot;, \u0026quot;bounds\u0026quot;: [ 0.0, 1.0 ] }, } } Load Data into Boxer #  "}),a.add({id:17,href:'/docs/publication/demo/',title:"Demo",section:"Publication",content:"Demo #  Demo website #  Click hereto open the online demo in a new browser tab.\nPublic Datasets #     Dataset Name Description Related Use case     Imputation The Mushroom dataset that considers a testing set of 2,000 (of 8,124) randomly selected instances. Model Selection   IMDB Confidence The data consists of 5044 movies with 27 features, however, 25% is sequestered for final assessment. A stratified sampling of 200 movies per class is held out from the 3756 for testing. Model Selection and Tuning   recid This dataset is used for fair learning: the Broward County recidivism dataset, popularized by ProPublica. The data set contains 6,172 instances and 14 numeric features (created by one-hot encoding the categorical features in the initial seven feature data set). 20% are held for testing. Fairness Assessment   date-12000-strat The dataset is the TCP collection of historical documents. It took a random sample of 12,000 documents, and held out 30% using stratified sampling. While the testing set is balanced (1,800 per class), the training set is highly skewed (only 15% before 1642) Bias and Data Discovery   fuzz-mod-5-02 The data set is a collection of 554 plays written in the Early Modern Period (1470-1660). Five linguistic features are used. It contains four kinds of plays : Comedy, History, Tragedy and Tragicomedy. Feature Sensitivity Testing   tcp-tree-select-9-10 This dataset considers a corpus of 59,989 documents from a historical literary collection and the data counts the 500 most common English words in each document. Model Selection and Data Discovery   (continuous) wine quality The dataset is used for wine quality classification, which requires classifying the quality of a wine from its properties. (Continuous) Hyper parameter Tuning   (continuous) income The dataset comes from income classification benchmark dataset from that has been downsampled. Classifiers determine whether an individual’s income is above a certain level. (Continuous) Model Selection   (continuous) cifar-sampled-scaling The datset is created based on CIFAR 100 computer vision benchmark using Tensorflow. The data set has 100 classes, and the trained classifier produces a distribution over these classes as its decision.A binary classifier has been created for a “meta-class” which combines 5 of the main classes. This datasets aims to classify flowers, which can be any one of 5 of the original classes. Because the test set contains all 100 classes, it is quite imbalanced: flowers are only 5% of the total instances (Continuous) Model Selection and Detail Examination   (continuous) cdate-2500 This dataset considers a corpus of 59,989 documents from a historical literary collection: Text Creation Partnership (TCP) transcriptions of the Early English Books Online (EEBO). The data counts the 500 most common English words in each document. For the experiment, we took a random sample of 2500 documents, and held out 30% using stratified sampling. (Continuous) Data Examination    "}),a.add({id:18,href:'/docs/user_guide/',title:"User Guide",section:"Docs",content:"User Guide #  We are in the process of developing a user guide to help people use Boxer.\nAt present there are four pieces:\n A Get started tutorial help you get familiar with the use of Boxer system A View descriptopn page help you get familiar with each view in Boxer system Instructions to prepare your own data for Boxer Views manipulation to help you make views, save / load views configurations.  The best way to learn how to use Boxer is probably to walk through the get started tutorial and read the guide in views manipulation. You can also learn the functions and detailed usage of each view in view descriptopn page.\nOnce you\u0026rsquo;re ready to try it yourself, look at the instructions on data preparation that will help you format your data so that it can be read by Boxer.\nFor more complex use, you could go to use cases that showcase different Boxer features on example data sets.\n"}),a.add({id:19,href:'/docs/use_cases/',title:"Use Case",section:"Docs",content:"More Use Cases #  This part consists of 6 use cases for Boxer and 4 use cases for CBoxer that are described in the papers. You can choose the scenarios you are interested in and then you will be guided to walk through of how we used boxer in use case.\n"})})()
<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction on My New Hugo Site</title>
    <link>/</link>
    <description>Recent content in Introduction on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bias and Data Discovery</title>
      <link>/docs/use_cases/case3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case3/</guid>
      <description>Bias and Data Discovery #    Dataset: The dataset for this use case is still the TCP collection of historical documents. This use case took a random sample of 12,000 documents, and held out 30% using stratified sampling. While the testing set is balanced (1,800 per class), the training set is highly skewed (only 15% before 1642).
  Classifiers: The classifiers have been constructed to determine whether a document is written after 1642 based on the 500 most common words in the corpus.</description>
    </item>
    
    <item>
      <title>Fairness Assessment</title>
      <link>/docs/use_cases/case2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case2/</guid>
      <description>Fairness Assessment #    Dataset: We consider a standard test case for fair learning: the Broward County recidivism dataset, popularized by ProPublica. The data set contains 6,172 instances and 14 numeric features (created by one-hot encoding the categorical features in the initial seven feature data set). 20% are held for testing.
  Classifiers: The task is to predict whether a person will commit a crime within two years (two-year recidivism).</description>
    </item>
    
    <item>
      <title>Feature Sensitivity Testing</title>
      <link>/docs/use_cases/case5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case5/</guid>
      <description>Feature Sensitivity Testing #    Dataset: The data set is a collection of 554 plays written in the Early Modern Period (1470-1660). Five linguistic features are used. We classify plays with one of four genres (Comedy, History, Tragedy and Tragicomedy).
  Classifiers: Our experiment uses a Support Vector Machine (SVM) classifier trained with class weights to counteract a skewed training distribution. A stratified sample of 20% was removed as the test set.</description>
    </item>
    
    <item>
      <title>Get Started</title>
      <link>/docs/user_guide/get_started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/user_guide/get_started/</guid>
      <description>Get started #  The tutorial will introduce the ways of comparing classfiers&#39; performance. It will guide you to load data, choose views and analyze visualizations.
1. Load dataset
Click the &amp;lsquo;+&amp;rsquo; at top left to choose the dataset (we use the &amp;lsquo;wine_quality&amp;rsquo; dataset for this tutorial).
2. Compare classifiers based on overall performance
There are multiple ways to compare the accuracy of each classifier. We can directly use Classifier Performance View, where each bar is composed of the correctly classified instances (represented by the blue bar at the bottom) and misclassified instances (represented by the orange bar on top).</description>
    </item>
    
    <item>
      <title>Model Selection</title>
      <link>/docs/use_cases/case6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case6/</guid>
      <description>Model Selection #    Dataset: This use case uses Mushroom dataset and considers a testing set of 2,000 (of 8,124) randomly selected instances.
  Classifiers: We consider adapting a classifier to identify poisonous mushrooms. The initial training of the baseline classifier uses the color feature, but we built a new classifier that does not use this feature. We consider two approaches to imputing the missing color feature by using the mode of the data and training a decision tree to classify the color based on other features.</description>
    </item>
    
    <item>
      <title>Model Selection and Data Discovery</title>
      <link>/docs/use_cases/case1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case1/</guid>
      <description>Model Selection and Data Discovery #    Dataset: This use case considers a corpus of 59,989 documents from a historical literary collection and the data counts the 500 most common English words in each document.
  Classifiers: The classifiers for this task are decision tree classifiers using a variety of univariate feature selection strategies, each selecting 10 words to count for features. The feature selection methods were: most relevant by a CHI-squared univariate feature selector (C), most common features (N), randomly chosen features (R), and, as a baseline, the features deemed worst (out of the 500 candidate words) by the CHI-squared test (W).</description>
    </item>
    
    <item>
      <title>Model Selection and Tuning</title>
      <link>/docs/use_cases/case4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case4/</guid>
      <description>Model Selection and Tuning #    Dataset: The data consists of 5044 movies with 27 features, however, 25% is sequestered for final assessment. A stratified sampling of 200 movies per class is held out from the 3756 for testing.
  Classifiers: We develop a classifier to predict movie ratings from IMDB. Classifiers predict a movie’s rating (low, medium, high). A variety of classifiers were constructed such as linear regression(lr), k- neural network(knn), support vector machines(svm), neural network(nn), random forest(rf) and linear discriminant analysis(lda), none with acceptable accuracy.</description>
    </item>
    
    <item>
      <title>Overall Introduction</title>
      <link>/docs/introduction/overall_introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/introduction/overall_introduction/</guid>
      <description>Overall Introduction #  Boxer #  Motivation #  Machine learning practitioners often perform experiments that compare classification results. Users gather the results of different classifiers and/or data perturbations on a collection of testing examples. Results data are stored and analyzed for tasks such as model selection, hyper-parameter tuning, data quality assessment, fairness testing, and gaining insight about the underlying data. Classifier comparison experiments are typically evaluated by summary statistics of model performance, such as accuracy, F1, and related metrics.</description>
    </item>
    
    <item>
      <title>Paper</title>
      <link>/docs/publication/paper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/publication/paper/</guid>
      <description>Paper #  To cite boxer:
 Michael Gleicher, Aditya Barve, Xinyu Yu, and Florian Heimerl. Boxer: Interactive Comparison of Classifier Results. Computer Graphics Forum 39 (3), June 2020. DOI:10.1111/cgf.13972
 Official journal web page (the paper is open access!): https://diglib.eg.org/handle/10.1111/cgf13972</description>
    </item>
    
    <item>
      <title>Views Manipulation</title>
      <link>/docs/user_guide/other_usage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/user_guide/other_usage/</guid>
      <description>Additional guide for views manipulation #  This tutorial will walk through some basic usages of Boxer system, like how to make views, save / reload views configuration.
Make views #  Save and reload configuration #  Clicking the &amp;ldquo;Save Views&amp;rdquo; button enables us to save the configurations of current views. After clicing the button, a &amp;ldquo;views_config.json&amp;rdquo; file will be created and it contains the important configurations of all views, including the view&amp;rsquo;s mode, the selected options in each view, the thresholds of classifiers, etc.</description>
    </item>
    
    <item>
      <title>(Continuous) Hyper parameter Tuning</title>
      <link>/docs/use_cases/case7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case7/</guid>
      <description>Hyper parameter Tuning #    Dataset: The scenario uses wine quality benchmark from UCI Machine Learnng Repository, which requires classifying the quality of a wine from its properties.
  Classifiers: Because we are trying to determine a hyper-parameter, we perform this analysis using the training data. We build a random forest classifier, and find that it gives good performance over a range of thresholds. We want to understand if changes in the threshold affect the outcomes</description>
    </item>
    
    <item>
      <title>Descriptions of Views</title>
      <link>/docs/user_guide/view_description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/user_guide/view_description/</guid>
      <description>Description of views #  Overview of Boxer Views #  Boxer features a number of different views that provide often needed arrangements of boxes useful in performing classifier comparison. This part will provide you with an overview of the views in Boxer. You will learn the basic uses of each view.
To add a view to the workspace, use the Views dropdown in the right panel of Boxer&amp;rsquo;s window. It is below the &amp;ldquo;filters&amp;rdquo; panel.</description>
    </item>
    
    <item>
      <title>Video</title>
      <link>/docs/publication/video/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/publication/video/</guid>
      <description>Boxer Videos #  Boxer (Submission) Video #  Continuous Boxer Video #  to be updated&amp;hellip;</description>
    </item>
    
    <item>
      <title>(Continuous) Data Examination</title>
      <link>/docs/use_cases/case8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case8/</guid>
      <description>Data Examination #    Dataset: This use case considers a corpus of 59,989 documents from a historical literary collection: Text Creation Partnership (TCP) transcriptions of the Early English Books Online (EEBO). The data counts the 500 most common English words in each document. For the experiment, we took a random sample of 2500 documents, and held out 30% using stratified sampling.
  Classifiers: We construct classifiers that determine whether a document is written after 1642 based on the 500 most common words in the corpus.</description>
    </item>
    
    <item>
      <title>(Continuous) Model Selection</title>
      <link>/docs/use_cases/case9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case9/</guid>
      <description>Data Examination #    Dataset: The scenario uses the income classification benchmark dataset from that has been downsampled.
  Classifiers: Classifiers determine whether an individual’s income is above a certain level. Three models were constructed with different methods: multi-layer perceptron (MLP), logistic regression (LR), and Na¨ıve Bayes (NB).
  Walkthrough: From Performance Overall view, the 3 models provide very similar correctness summary scores (F1 is .783, identical to 3 decimal places) over a test set of 1500 items.</description>
    </item>
    
    <item>
      <title>(Continuous) Model Selection and Detail Examination</title>
      <link>/docs/use_cases/case20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/use_cases/case20/</guid>
      <description>Model Selection and Detail Examinationt #  This use case shows how our approach can help with model selection and threshold tuning. A classifier was built for the CIFAR 100 computer vision benchmark using Tensorflow.
  Dataset: In this example, we have a dataset of various flower images. The dataset has 100 classes. Our goal is to create a binary classifier for a “meta-class” which combines 5 of the main classes.</description>
    </item>
    
    <item>
      <title>Data Preparation</title>
      <link>/docs/user_guide/data_preparation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/user_guide/data_preparation/</guid>
      <description>Instructions to prepare your own data for Boxer #  CORS #  In order to successfully load your dataset, you need to first add an extension called CORS (Access-Control-Allow-Origin lets you easily perform cross-domain Ajax requests in web applications) to Chrome. Everytime you want to use your own dataset, please remember to enable &amp;lsquo;CORS&amp;rsquo;. Data Set Composition #  The data set used for Boxer consists of 3 files:</description>
    </item>
    
    <item>
      <title>Demo</title>
      <link>/docs/publication/demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/publication/demo/</guid>
      <description>Demo #  Demo website #  Click hereto open the online demo in a new browser tab.
Public Datasets #     Dataset Name Description Related Use case     Imputation The Mushroom dataset that considers a testing set of 2,000 (of 8,124) randomly selected instances. Model Selection   IMDB Confidence The data consists of 5044 movies with 27 features, however, 25% is sequestered for final assessment.</description>
    </item>
    
  </channel>
</rss>
